
#+HUGO_BASE_DIR: ../../
#+HUGO_SECTION: posts

#+TITLE: Personal blog

* Converting openssh private key to PEM format :openssh:
  :PROPERTIES:
  :EXPORT_FILE_NAME: openssh-private-key
  :EXPORT_DATE: 2020-03-29
  :END:
  Newer versions of the =openssh= will use their own format to store a private
  key:

  #+begin_src conf
    -----BEGIN OPENSSH PRIVATE KEY-----
    BASE64KEYTEXT
    -----END OPENSSH PRIVATE KEY-----
  #+end_src
  
  The traditional format of the key can sometimes be useful (or even required -
  for example [[https://guacamole.apache.org/][guacamole]] will not be able to use the =OPENSSH= key format).

  To convert the key use =ssh-keygen=. The command will overwrite the private
  key file, so if the original key should be preserved make sure to create a
  backup:
  
  #+begin_src bash
    ssh-keygen -p -m PEM -f ~/.ssh/id_rsa_test
  #+end_src

  #+name: Output
  #+begin_src conf
    -----BEGIN RSA PRIVATE KEY-----
    BASE64KEYTEXT
    -----END RSA PRIVATE KEY-----
  #+end_src

* Deploying Teamspeak on a Raspberry PI kubernetes cluster :kubernetes:arm:raspberry:
  CLOSED: [2020-05-08 Fri 22:59]
  :PROPERTIES:
  :EXPORT_FILE_NAME: teamspeak-k8s-arm
  :EXPORT_DATE: 2020-04-18
  :END:
  While this post will end up with a running [[https://www.teamspeak.com/en/][Teamspeak]] server, it is very hard
  on the resources of the Raspberry Pi and might not be suitable for everyday
  use.
  
  To deploy a teamspeak server on raspberry pi a few things need to be done:

  - (optional) Get a Kubernetes cluster up and running (this is not required, if
    you just want to run the =docker= container directly).
  - Get Teamspeak to run on =ARM=.
  - Setup an ingress controller to make the Teamspeak server accessible from the
    outside world (in this case this will be the [[https://kubernetes.github.io/ingress-nginx/][Nginx Ingress]]).
** Setting up a kubernetes cluster  
   To setup a kubernetes cluster on Raspberry Pis [[https://k3s.io/][K3S]] is very good approach, as
   the cluster will be more lightweight that simply installing upstream
   kubernetes.

   As a base I recommend using [[https://blog.hypriot.com/][HypriotOS]] or [[https://ubuntu.com/download/server/arm][Ubuntu Server]][fn:3] as those allow
   configuring the images using [[https://cloudinit.readthedocs.io/en/latest/][Cloud-Init]].
   
   Make sure to follow the instructions to use the the =legacy= backend for
   =iptables= if installing =kubernetes= v1.17 or lower: [[https://v1-17.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#ensure-iptables-tooling-does-not-use-the-nftables-backend][kubeadm instructions]].
   
   When installing =k3s= to run =Teamspeak= =Traefik= should not be installed,
   as only the 2.x version supports =UDP= ingresses - so instead =nginx-ingress=
   will be installed later:

   #+begin_src sh
     curl -sfL https://get.k3s.io | sh -s - --no-deploy=traefik
   #+end_src
** Deploy Teamspeak
*** Build an ARM image for Teamspeak
    Teamspeak does not provide a binary for =ARM=.

    It is however possible to run it using [[https://www.qemu.org/][Qemu]] - I have already prepared an
    =ARM= image that will run the Teamspeak server through =qemu= that you can
    find on my [[https://hub.docker.com/repository/docker/monadt/teamspeak3-server][Dockerhub]] - or if you want to see the source checkout the
    repository on [[https://github.com/bergmannf/teamspeak3-server-arm][Github]].
    
    The image is using the same =entrypoint.sh= as the [[https://hub.docker.com/_/teamspeak][official image]] - so if
    you are already using that one you should be able to use it exactly the same
    way (if not - feel free to open an issue).
*** Deploy the image
    Now - if you do not want to use =kubernetes=, you can simply use the image
    using =docker= and expose the required =Teamspeak= ports as you would with
    =docker=:

    #+begin_src sh
      docker run -p 9987:9987/udp -p 10011:10011 -p 30033:30033 -e TS3SERVER_LICENSE=accept monadt/teamspeak3-server
    #+end_src

    This way is a lot easier on the resources and will likely run more reliable
    on a Raspberry PI with < 2 GB of RAM.
** Setup nginx-ingress 
*** Get nginx-ingress to run on ARM
   Setting up the =nginx-ingress= on a cluster running on =ARM= needs a few extra
   steps when using the [[https://kubernetes.github.io/ingress-nginx/deploy/][official documentation]].
  
   The images used in the =manifests= are not compatible with =armv7= (that is
   used when running a cluster on a bunch of Raspberry Pis).
  
   First the =mandatory.yaml= has to be updated to use the images for the =arm=
   architecture[fn:1]:

   #+begin_src sh
     wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml
     sed -i 's/nginx-ingress-controller:0/nginx-ingress-controller-arm:0/' mandatory.yaml
   #+end_src
  
   The resulting =mandatory.yaml= file can now be applied to the cluster:

   #+begin_src sh
     kubectl apply -f mandatory.yaml
   #+end_src
  
   In my local cluster I am using the =NodePort= approach, so the service for
   that can be applied next:
  
   #+begin_src sh
     kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml
   #+end_src
** Setup a Teamspeak deployment
   With all pieces in place the =Teamspeak= container can now be deployed onto
   the cluster:
   
   Save the following =yaml= into a file (e.g. =teamspeak.yaml=).
   #+begin_src yaml
     ---
     apiVersion: v1
     kind: Namespace
     metadata:
       name: teamspeak
     ---
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: teamspeak-pvc
       namespace: teamspeak
     spec:
       accessModes:
         - ReadWriteOnce
       storageClassName: local-path
       resources:
         requests:
           storage: 256Mi
     ---
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: teamspeak-deployment
       namespace: teamspeak
       labels:
         app: teamspeak
     spec:
       replicas: 1
       selector:
         matchLabels:
           app: teamspeak
       template:
         metadata:
           namespace: teamspeak
           labels:
             app: teamspeak
         spec:
           containers:
             - name: teamspeak-server
               image: monadt/teamspeak3-server:3.11.0
               ports:
                 - name: ts
                   containerPort: 9987
                   protocol: UDP
               resources:
               env:
               - name: TS3SERVER_LICENSE
                 value: accept
               volumeMounts:
               - mountPath: /var/ts3server/
                 name: teamspeak-data
           volumes:
             - name: teamspeak-data
               persistentVolumeClaim:
                 claimName: teamspeak-pvc
     ---
     apiVersion: v1
     kind: Service
     metadata:
       name: teamspeak-service
       namespace: teamspeak
       labels:
         app: teamspeak
     spec:
       type: ClusterIP
       ports:
         - port: 9987
           targetPort: ts
           protocol: UDP
           name: ts
       selector:
         app: teamspeak
     ---
     apiVersion: v1
     kind: ConfigMap
     metadata:
       name: udp-services
       namespace: ingress-nginx
     data:
       9987: "teamspeak/teamspeak-service:9987"
   #+end_src
    
   Apply this using =kubectl=:

   #+begin_src sh
     kubectl apply -f teamspeak.yaml
   #+end_src
   
   The 9987 =udp= port will also need to be added to the =ingress= service.
   In the =ports= section of the service add the following snippet:

   #+begin_src sh
     kubectl edit svc ingress-nginx -n ingress-nginx
   #+end_src
   
   #+begin_src yaml
       - name: teamspeak
         port: 9987
         protocol: UDP
         targetPort: 9987
   #+end_src

** Forwarding traffic to the ingress
   The final step depends a lot on the setup you are deploying the cluster in.

   If it is behind your local router, you have to check which port was bound to
   the 9987 =udp= port and forward this to one of your cluster-nodes:

   #+begin_src sh
     kubectl describe svc ingress-nginx -n ingress-nginx
   #+end_src
   
   #+begin_src text
     Name:                     ingress-nginx
     Namespace:                ingress-nginx
     Labels:                   app.kubernetes.io/name=ingress-nginx
                               app.kubernetes.io/part-of=ingress-nginx
     Annotations:              Selector:  app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx
     Type:                     NodePort
     IP:                       10.43.33.70
     Port:                     http  80/TCP
     TargetPort:               80/TCP
     NodePort:                 http  32224/TCP
     Endpoints:                
     Port:                     https  443/TCP
     TargetPort:               443/TCP
     NodePort:                 https  31955/TCP
     Endpoints:                
     Port:                     teamspeak  9987/UDP
     TargetPort:               9987/UDP
     NodePort:                 teamspeak  31222/UDP
     Endpoints:                
     Session Affinity:         None
     External Traffic Policy:  Cluster
     Events:                   <none>
   #+end_src
   
   In this case the port that needs to be forwarded is the =31222= port (the
   =NodePort= for the 9987 =UDP= port).
* Testing an ansible collection                                     :ansible:
  :PROPERTIES:
  :EXPORT_FILE_NAME: testing-ansible-collections
  :EXPORT_DATE: 2020-05-08
  :END:
  When wanting to work on a collection for =ansible= it is (for now[fn:2]) important
  to check it out in a very specific folder structure or it will not be possible
  to run the tests for it.
  
  When trying to run =ansible-test integration= it will otherwise throw the
  following error:

  #+begin_src text
    ERROR: The current working directory must be at or below:

     - an Ansible collection: {...}/ansible_collections/{namespace}/{collection}/

    Current working directory: <some_other_dir>
  #+end_src

  The collection must be really placed inside a =subfolder= of a folder called
  =ansible_collections= and neither =namespace= nor =collection= can contain
  any symbols except =alphanumberics= or =underscores= (=[a-zA-Z0-9_]=):

  #+begin_src sh
    ansible_collections
    └── namespace
        └── collection
  #+end_src
  
  So if you want to work on an upstream collection (e.g. [[https://github.com/ansible-collections/community.general][community.general]]) you
  should create an intermediate folder =community= and clone the collection into
  the =general= folder (contrary to the default checkout which would be
  =community.general=):
  
  #+begin_src sh
    ansible_collections
    └── community
        └── general
  #+end_src
  
  Inside =general= you can now use run =ansible-test integration= to run
  the integration tests successfully:
  
  #+begin_src sh
    cd ansible_collections/community/general
    poetry init --name community.general --dependency=ansible --dependency=pyyaml --dependency=jinja2 -n
    poetry install
    poetry run ansible-test integration
  #+end_src
* Install kata-containers on K3S on aarch64            :kubernetes:raspberry:
  :PROPERTIES:
  :EXPORT_FILE_NAME: kata_containers_raspberry
  :EXPORT_DATE: 2020-05-21
  :END:
  To install =kata-containers= on a =raspberry-pi= and integrating it into
  =kubernetes= a few steps are currently required:

  - Install =kata-containers=
  - Setup integration into =containerd=
  - Setup integration into =kubernetes=
** Install =kata-containers=
   Currently the easiest (and apparently only) officially supported way to
   install =kata-containers= on an =aarch64= system is to use =snaps=.

   If you are using the =Ubuntu= =arm= version this is already included in the
   installation and you can install =kata-containers= with a single command[fn:4]:

   #+begin_src sh
     sudo snap install kata-containers --classic
   #+end_src
   
** Integration into containerd
   When running =k3s= there is no =containerd= binary as it is embedded into
   =k3s=, so configuration must be performed in the =k3s= configuration[fn:5].
   
   The whole process includes the following steps:

   - A =runtime= must be configured for =kata-containers= in =containerd=.
   - =shims= must be created for =containerd= to use =kata-containers=.
   - The node must be configured to be able to run =kata-container= workloads.
   - The kubernetes cluster must be aware that =kata= is a valid runtime.
   
   To configure a =runtime=, the configuration file for =k3s= is found under:
   =/var/lib/rancher/k3s/agent/etc/containerd/config.toml=, however it can not
   be edited as it will be regenerated every time =k3s= is restarted. Instead it
   [[https://rancher.com/docs/k3s/latest/en/advanced/][should be copied]] to
   =/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl= and that file
   can be modified.
   
   #+begin_src sh
     cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml{,.tmpl}
   #+end_src
   
   The content should be:

   #+begin_src toml
     [plugins.cri.containerd.runtimes.kata]
       runtime_type = "io.containerd.kata.v2"

     [kata.options]
       ConfigPath = "/etc/kata-containers/configuration.toml"
   #+end_src
   
   The configuration files for =/etc/kata-containers/*.toml= must be copied from
   the snap, because the default configuration will not be able to start on
   Raspberry Pis with < 4GB of memory, as =qemu= requests too much memory by
   default:

   #+begin_src sh
     mkdir -p /etc/kata-containers/
     cp /snap/kata-containers/current/usr/share/defaults/kata-containers/*.toml /etc/kata-containers/
   #+end_src
   
   Inside these configuration files (depending on the Pi) make sure to adjust the
   value of =default_memory= to something the hardware can handle. You should at
   least adjust the default =configuration.toml= and =configuration-qemu.toml=.
   
   In addition =shim= files must be created in =/usr/local/bin= to point to the
   =shim= binaries of =kata-containers= for =containerd= to pick it up.
   
   The following script will create all the shims (including some that might not
   actually be supported) - it was copied out of the =kata-deploy= project.

   #+begin_src sh
     #!/bin/bash
     shims=(
         "fc"
         "qemu"
         "qemu-virtiofs"
         "clh"
     )

     for shim in "${shims[@]}"; do
         shim_binary="containerd-shim-kata-${shim}-v2"
         shim_file="/usr/local/bin/${shim_binary}"
         shim_backup="/usr/local/bin/${shim_binary}.bak"

         if [ -f "${shim_file}" ]; then
             echo "warning: ${shim_binary} already exists" >&2
             if [ ! -f "${shim_backup}" ]; then
                 mv "${shim_file}" "${shim_backup}"
             else
                 rm "${shim_file}"
             fi
         fi
         cat << EOT | tee "$shim_file"
     #!/bin/bash
     KATA_CONF_FILE=/etc/kata-containers/configuration.toml /snap/kata-containers/current/usr/bin/containerd-shim-kata-v2 \$@
     EOT
         chmod +x "$shim_file"
     done

     # On the PI a default shim was also needed
     cp /usr/local/bin/containerd-shim-kata-qemu-v2 /usr/local/bin/containerd-shim-kata-v2
   #+end_src
   
   Now the =k3s-agent= must be restarted to reload the =containerd=
   configuration and we can test that kata-container runtime is working by
   running the following commands:

   #+begin_src sh
     systemctl restart k3s-agent
     ctr image pull docker.io/library/busybox:latest
     ctr run --runtime io.containerd.run.kata.v2 -t --rm docker.io/library/busybox:latest hello sh
   #+end_src
   
   If the tests succeed the node can now be labeled as supporting =kata-containers=:

   #+begin_src sh
     kubectl label node "$NODE_NAME" --overwrite katacontainers.io/kata-runtime=true
   #+end_src
   
   Now as a last step a new [[https://kubernetes.io/docs/concepts/containers/runtime-class/][RuntimeClass]] must be created for =kata= that can be
   used to force pods to be run using it:

   #+begin_src yaml
     apiVersion: node.k8s.io/v1beta1
     kind: RuntimeClass
     metadata:
       name: kata
     handler: kata
   #+end_src
   
   Now pods can use this runtime by specifying =runtimeClassName= inside the =spec=:

   #+begin_src yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: nginx-untrusted
     spec:
       runtimeClassName: kata
       containers:
       - name: nginx
         image: nginx
   #+end_src
* Debugging kubernetes containers :kubernetes:
:PROPERTIES:
:EXPORT_FILE_NAME: debugging_kubernetes_containers
:EXPORT_DATE: 2023-12-01
:END:

The scenario: you have a running kubernetes cluster, but suddenly some of your
containers start to have problems.

Obviously now there must be a way to find out what exactly is causing these
problems.

In this post I'll highlight two ways to debug a container running on kubernetes:

- =ephemeral-containers= (which is likely what you should use most of the time)
- =nsenter= (which is a last resort debugging option that requires access to the
  node the Pod is running on)

#+begin_quote
Want to follow along? Setup a test cluster and run the provided command to get
your very own failing container:

#+begin_src yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: myconfig
  namespace: default
data:
  config.toml: |
    [default]
  additional_file: ""
---
apiVersion: v1
kind: Pod
metadata:
  name: my-broken-pod
  namespace: default
spec:
  containers:
  - name: working-container
    image: gcr.io/google_containers/pause-amd64:3.0
  - name: broken-container
    image: quay.io/fbergman/broken-pod:latest
    imagePullPolicy: IfNotPresent
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
    ports:
      - containerPort: 3000
    readinessProbe:
      httpGet:
        path: /readyz
        port: 3000
    volumeMounts:
      - mountPath: /opt/
        name: configuration
  volumes:
    - name: configuration
      configMap:
        name: myconfig
#+end_src

#+begin_src sh
minikube start
kubectl apply -f /some-web-url
#+end_src
#+end_quote

In this case one of the =LivenessProbes= starts failing.

In case of one container (=working-container=), there is no way to debug into
it, because it does not contain any executable shell. This is a pretty common
scenario when running minimal container images like [[https://github.com/GoogleContainerTools/distroless][distroless]] containers.

The other container allows spawning a shell, but is missing most utilities that
would make debugging easier: there is no =strace=, no =gdb= and no default
networking tools.

How can we now proceed to figure out what breaks the container starting?

Obviously we can hope that a =kubectl describe pod= or the logs of our
application has some useful information, but let's say this is not the case and
running =kubectl logs my-broken-pod= simply returns nothing useful.

In this case the two already mentioned approaches can be used - as it should be
your first stop let's start with *ephemeral containers*:

** Using ephemeral containers
When having to debug a pod, ephemeral containers are the frist tool to use,
because in general access to cluster nodes might not be available.

When using ephemeral containers it is important to have some container images
ready that contain tools you want to use for debugging.

A very good network debugging container is the [[https://github.com/nicolaka/netshoot][netshoot]] container.

But if all you need is a basic shell (in case of a distroless container) -
busybox might already be enough.
*** Starting an ephemeral container
Ephemeral containers are handled as an additional field in the =spec= of a
container: [[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/#ephemeralcontainer-v1-core][see the apidocs]].

Ephemeral containers can only be added to an already running pod - trying to add
them to a Pod during creation will result in an error:

#+begin_src yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-container-on-start
  labels:
    name: ephemeral-container-on-start
spec:
  containers:
  - name: main-container
    image: busybox:1.28
    resources:
      limits:
        memory: "64"
        cpu: "250m"
  ephemeralContainers:
  - name: debug-container
    image: busybox:1.28
    resources:
      limits:
        memory: "64Mi"
        cpu: "250m"
#+end_src

#+begin_src sh
The Pod "ephemeral-container-on-start" is invalid:
,* spec.ephemeralContainers[0].resources: Forbidden: cannot be set for an Ephemeral Container
,* spec.ephemeralContainers: Forbidden: cannot be set on create
#+end_src

That leaves two options to add an ephemeral container to a Pod:

1. The =kubectl debug= command:
   #+begin_src sh
kubectl debug -it [pod-name] --image=busybox:1.28 --target=[container-name]
   #+end_src
2. Patching the Pod using the API directly using the new resource:
   #+begin_src sh
# Just for this experiment, give the default serviceaccount all permissions
kubectl create clusterrolebinding --clusterrole cluster-admin --serviceaccount default:default default-all-permissions
# APIURL
APIURL=$(kubectl config view --minify --output jsonpath="{.clusters[*].cluster.server}")
# Impersonate a serviceaccount with the required privileges
TOKEN=$(kubectl create token default)
# Call the API using curl as this serviceaccount
curl --header "Authorization: Bearer $TOKEN" \
    --header "Content-Type: application/strategic-merge-patch+json" \
    --request PATCH \
    -d '
{
    "spec":
    {
        "ephemeralContainers":
        [
            {
                "name": "debugger",
                "command": ["sh"],
                "image": "busybox",
                "targetContainerName": "broken-container",
                "stdin": true,
                "tty": true,
                "volumeMounts": []
            }
        ]
    }
}' \
    "${APIURL}/api/v1/namespaces/default/pods/my-broken-pod/ephemeralcontainers" -k
   #+end_src

   Once the container has been added like that it can be used using =kubectl
   exec -ti my-broken-pod -c debugger sh=. Getting rid of the new container can
   be done, by killing the process that was already running when =execing= into
   the container: =kill -9 <pid_of_initial_sh>=.

What will not work is using =kubectl edit=, as this is [[https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/#what-is-an-ephemeral-container][explicitly not supported]]:

#+begin_quote
Ephemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to pod.spec, so it's not possible to add an ephemeral container using kubectl edit.
#+end_quote

Once the container is launched (and you either already are inside the shell when
using =kubectl debug= or have attached via =kubectl exec=) you can run whatever
commands might be necessary.
*** Cleaning up ephemeral containers
Checking the apidocs also shows what might be an issue for some user:

#+begin_quote
Ephemeral containers may not be removed or restarted.
#+end_quote

So - once a ephemeral container has been added to a Pod, the only way to
completely get rid of it from the Pod spec is by recreating the Pod without it.

It's also strongly recommended to not keep a process running in the ephemeral
container, because it will work against the requests and limits of the entire
pod:

#+begin_quote
The kubelet may evict a Pod if an ephemeral container causes the Pod to exceed its resource allocation.
#+end_quote
** Using nsenter
Using =nsenter= will not only work for kubernetes pods, so this is something you
can also use for =docker= or =podman= containers.

Generally this is useful if you want to pick-and-choose which *namespaces* of
the container you want to access.

#+begin_quote
In kubernetes this will only work, if there is a way to gain access to the
cluster node the Pod you want to debug is running on.

So either via =ssh= or using a [[https://www.redhat.com/sysadmin/how-oc-debug-works][openshift debug node/NODE]] container.
#+end_quote

Once access to the node is established the process running inside the pod needs
to be found: a lot of these commands depend on the container runtime in use, so
for the remainder I will assume the cluster is using =CRIO=.[fn:7]

1. Find the Pod ID for our =my-broken-pod= pod:
  #+begin_src sh
POD_ID=$(crictl pods --name my-broken-pod -o json | jq -r '.items[] | select(.state == "SANDBOX_READY") | .id')
  #+end_src
2. Find all processes inside this pod:
  #+begin_src sh
CONTAINERS=$(crictl ps --pod d718ccb51d73896035324f6d9d9d12cc6818027ab18e5f78b295b518c62b46bd -o json | jq -r '.containers[].id')
  #+end_src
3. Find all processes inside these containers (or just choose the one container that is running the crashing process):
  #+begin_src sh
crictl inspect 154c67e44fcc843201aa582214395c82eb0e40acfda1ef1a9e12567f371aa13d | jq -r ".info.pid"
  #+end_src

With the =PID= it is now possible to enter some of the namespaces of this
process, while maintaining access to all binaries installed on the machine we
SSHed to (as long as the =mount= namespace is *not* used with =nsenter= -
indicated by the =-m= flag):

#+begin_src sh
PID=1234
nsenter -t $PID -n -p -u
#+end_src

Often it can be useful to add [[https://man7.org/linux/man-pages/man7/namespaces.7.html][namespaces]] incrementally to see if one of the
namespaces might have an impact on the container's behaviour.

In this case it's now possible to inspect the process that is refusing to work
with all tools available on the host - in case of a =minikube= cluster this
includes tools like =lsof= and =strace=.

But first let's check if the =ReadinessProbe= works right now:

#+begin_src sh
curl -I localhost:3000/readyz
#+end_src

#+begin_src text
HTTP/1.1 500 Internal Server Error
content-type: text/plain; charset=utf-8
content-length: 7
date: Fri, 01 Dec 2023 11:16:12 GMT
#+end_src

So it is returning a *500* error - maybe we can get some more information what
the process is actually doing using strace:

#+begin_src sh
strace -f -p "$PID"
#+end_src

#+begin_src text
[pid 128904] epoll_wait(3,  <unfinished ...>
[pid 128893] futex(0x7f6985695940, FUTEX_WAIT_PRIVATE, 1, NULL <unfinished ...>
[pid 128904] <... epoll_wait resumed>[], 1024, 202) = 0
[pid 128904] epoll_wait(3, [], 1024, 17) = 0
[pid 128904] statx(AT_FDCWD, "/opt/config/additional_file", AT_STATX_SYNC_AS_STAT, STATX_ALL, {stx_mask=STATX_ALL|STATX_MNT_ID, stx_attributes=0, stx_mode=S_IFREG|0644, stx_size=0, ...}) = 0
[pid 128904] write(1, "app_state bad - configuration at"..., 61) = 61
[pid 128904] write(4, "\1\0\0\0\0\0\0\0", 8) = 8
#+end_src

It seems to =stat= a configuration file at =/opt/config/additional_file= -
that's strange, as the configuration should live in another file
(=config.toml=)[fn:6].

Time to actually use the mount namespace and see what's in =/opt/config=

#+begin_src sh
nsenter -t $PID -n -p -u -m
ls /opt/config
#+end_src

#+begin_src sh
additional_file  config.toml
#+end_src

Well the file is there - so let's update our configuration to not contain it and
see if it unbreaks our application:

#+begin_src sh
kubectl patch -n default configmaps myconfig --type=json -p '[{"op": "remove", "path": "/data/additional_file"}]'
#+end_src

After removing that key from the =ConfigMap= and a Pod restart it seems our
application is finally happy!

#+begin_src text
my-broken-pod   2/2     Running   0          10s
#+end_src

#+begin_quote
Obviously in this case the =strace= command could also have been run directly
from the node, but knowing how to incrementally add container namespaces can
still be useful - e.g. to check how a mounted filesystem really looks like
inside the container.
#+end_quote
** References
- [[https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/][Ephemeral containers (official documentation)]]
- [[https://iximiuz.com/en/posts/kubernetes-ephemeral-containers/][Ephemeral containers (great blog post)]]
- [[https://man7.org/linux/man-pages/man1/nsenter.1.html][nsenter man page]]
* Footnotes
[fn:7] If the process is easy to identify running =ps= and grepping for the
process commandline will also work.

[fn:6] Obviously in this example this file was mounted from the =ConfigMap= - in
reality this might indicate problems with other software that might inject
libraries into all processes via =LD_PRELOAD= modifications.

[fn:5] Most information in this section is a direct extract from [[https://github.com/kata-containers/packaging/pull/823][this PR]] adding =k3s= support to =kata-deploy=.

[fn:4] As documented [[https://github.com/kata-containers/packaging/blob/master/snap/README.md#initial-setup][here]].

[fn:3] For Ubuntu you will have to edit the file =/boot/firmware/cmdline.txt=
and add the options =cgroup_memory=1 cgroup_enable=memory= at the end of the
line for =k3s= (or =containers= in general) to work.

[fn:2] This might hopefully become easier in the future: https://github.com/ansible/ansible/issues/60215

[fn:1] See https://github.com/kubernetes/ingress-nginx/pull/3852 
