
#+HUGO_BASE_DIR: ../../
#+HUGO_SECTION: posts

#+TITLE: Personal blog

* Converting openssh private key to PEM format :openssh:
  :PROPERTIES:
  :EXPORT_FILE_NAME: openssh-private-key
  :EXPORT_DATE: 2020-03-29
  :END:
  Newer versions of the =openssh= will use their own format to store a private
  key:

  #+begin_src conf
    -----BEGIN OPENSSH PRIVATE KEY-----
    BASE64KEYTEXT
    -----END OPENSSH PRIVATE KEY-----
  #+end_src
  
  The traditional format of the key can sometimes be useful (or even required -
  for example [[https://guacamole.apache.org/][guacamole]] will not be able to use the =OPENSSH= key format).

  To convert the key use =ssh-keygen=. The command will overwrite the private
  key file, so if the original key should be preserved make sure to create a
  backup:
  
  #+begin_src bash
    ssh-keygen -p -m PEM -f ~/.ssh/id_rsa_test
  #+end_src

  #+name: Output
  #+begin_src conf
    -----BEGIN RSA PRIVATE KEY-----
    BASE64KEYTEXT
    -----END RSA PRIVATE KEY-----
  #+end_src

* Deploying Teamspeak on a Raspberry PI kubernetes cluster :kubernetes:arm:raspberry:
  CLOSED: [2020-05-08 Fri 22:59]
  :PROPERTIES:
  :EXPORT_FILE_NAME: teamspeak-k8s-arm
  :EXPORT_DATE: 2020-04-18
  :END:
  While this post will end up with a running [[https://www.teamspeak.com/en/][Teamspeak]] server, it is very hard
  on the resources of the Raspberry Pi and might not be suitable for everyday
  use.
  
  To deploy a teamspeak server on raspberry pi a few things need to be done:

  - (optional) Get a Kubernetes cluster up and running (this is not required, if
    you just want to run the =docker= container directly).
  - Get Teamspeak to run on =ARM=.
  - Setup an ingress controller to make the Teamspeak server accessible from the
    outside world (in this case this will be the [[https://kubernetes.github.io/ingress-nginx/][Nginx Ingress]]).
** Setting up a kubernetes cluster  
   To setup a kubernetes cluster on Raspberry Pis [[https://k3s.io/][K3S]] is very good approach, as
   the cluster will be more lightweight that simply installing upstream
   kubernetes.

   As a base I recommend using [[https://blog.hypriot.com/][HypriotOS]] or [[https://ubuntu.com/download/server/arm][Ubuntu Server]][fn:3] as those allow
   configuring the images using [[https://cloudinit.readthedocs.io/en/latest/][Cloud-Init]].
   
   Make sure to follow the instructions to use the the =legacy= backend for
   =iptables= if installing =kubernetes= v1.17 or lower: [[https://v1-17.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#ensure-iptables-tooling-does-not-use-the-nftables-backend][kubeadm instructions]].
   
   When installing =k3s= to run =Teamspeak= =Traefik= should not be installed,
   as only the 2.x version supports =UDP= ingresses - so instead =nginx-ingress=
   will be installed later:

   #+begin_src sh
     curl -sfL https://get.k3s.io | sh -s - --no-deploy=traefik
   #+end_src
** Deploy Teamspeak
*** Build an ARM image for Teamspeak
    Teamspeak does not provide a binary for =ARM=.

    It is however possible to run it using [[https://www.qemu.org/][Qemu]] - I have already prepared an
    =ARM= image that will run the Teamspeak server through =qemu= that you can
    find on my [[https://hub.docker.com/repository/docker/monadt/teamspeak3-server][Dockerhub]] - or if you want to see the source checkout the
    repository on [[https://github.com/bergmannf/teamspeak3-server-arm][Github]].
    
    The image is using the same =entrypoint.sh= as the [[https://hub.docker.com/_/teamspeak][official image]] - so if
    you are already using that one you should be able to use it exactly the same
    way (if not - feel free to open an issue).
*** Deploy the image
    Now - if you do not want to use =kubernetes=, you can simply use the image
    using =docker= and expose the required =Teamspeak= ports as you would with
    =docker=:

    #+begin_src sh
      docker run -p 9987:9987/udp -p 10011:10011 -p 30033:30033 -e TS3SERVER_LICENSE=accept monadt/teamspeak3-server
    #+end_src

    This way is a lot easier on the resources and will likely run more reliable
    on a Raspberry PI with < 2 GB of RAM.
** Setup nginx-ingress 
*** Get nginx-ingress to run on ARM
   Setting up the =nginx-ingress= on a cluster running on =ARM= needs a few extra
   steps when using the [[https://kubernetes.github.io/ingress-nginx/deploy/][official documentation]].
  
   The images used in the =manifests= are not compatible with =armv7= (that is
   used when running a cluster on a bunch of Raspberry Pis).
  
   First the =mandatory.yaml= has to be updated to use the images for the =arm=
   architecture[fn:1]:

   #+begin_src sh
     wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml
     sed -i 's/nginx-ingress-controller:0/nginx-ingress-controller-arm:0/' mandatory.yaml
   #+end_src
  
   The resulting =mandatory.yaml= file can now be applied to the cluster:

   #+begin_src sh
     kubectl apply -f mandatory.yaml
   #+end_src
  
   In my local cluster I am using the =NodePort= approach, so the service for
   that can be applied next:
  
   #+begin_src sh
     kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml
   #+end_src
** Setup a Teamspeak deployment
   With all pieces in place the =Teamspeak= container can now be deployed onto
   the cluster:
   
   Save the following =yaml= into a file (e.g. =teamspeak.yaml=).
   #+begin_src yaml
     ---
     apiVersion: v1
     kind: Namespace
     metadata:
       name: teamspeak
     ---
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: teamspeak-pvc
       namespace: teamspeak
     spec:
       accessModes:
         - ReadWriteOnce
       storageClassName: local-path
       resources:
         requests:
           storage: 256Mi
     ---
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: teamspeak-deployment
       namespace: teamspeak
       labels:
         app: teamspeak
     spec:
       replicas: 1
       selector:
         matchLabels:
           app: teamspeak
       template:
         metadata:
           namespace: teamspeak
           labels:
             app: teamspeak
         spec:
           containers:
             - name: teamspeak-server
               image: monadt/teamspeak3-server:3.11.0
               ports:
                 - name: ts
                   containerPort: 9987
                   protocol: UDP
               resources:
               env:
               - name: TS3SERVER_LICENSE
                 value: accept
               volumeMounts:
               - mountPath: /var/ts3server/
                 name: teamspeak-data
           volumes:
             - name: teamspeak-data
               persistentVolumeClaim:
                 claimName: teamspeak-pvc
     ---
     apiVersion: v1
     kind: Service
     metadata:
       name: teamspeak-service
       namespace: teamspeak
       labels:
         app: teamspeak
     spec:
       type: ClusterIP
       ports:
         - port: 9987
           targetPort: ts
           protocol: UDP
           name: ts
       selector:
         app: teamspeak
     ---
     apiVersion: v1
     kind: ConfigMap
     metadata:
       name: udp-services
       namespace: ingress-nginx
     data:
       9987: "teamspeak/teamspeak-service:9987"
   #+end_src
    
   Apply this using =kubectl=:

   #+begin_src sh
     kubectl apply -f teamspeak.yaml
   #+end_src
   
   The 9987 =udp= port will also need to be added to the =ingress= service.
   In the =ports= section of the service add the following snippet:

   #+begin_src sh
     kubectl edit svc ingress-nginx -n ingress-nginx
   #+end_src
   
   #+begin_src yaml
       - name: teamspeak
         port: 9987
         protocol: UDP
         targetPort: 9987
   #+end_src

** Forwarding traffic to the ingress
   The final step depends a lot on the setup you are deploying the cluster in.

   If it is behind your local router, you have to check which port was bound to
   the 9987 =udp= port and forward this to one of your cluster-nodes:

   #+begin_src sh
     kubectl describe svc ingress-nginx -n ingress-nginx
   #+end_src
   
   #+begin_src text
     Name:                     ingress-nginx
     Namespace:                ingress-nginx
     Labels:                   app.kubernetes.io/name=ingress-nginx
                               app.kubernetes.io/part-of=ingress-nginx
     Annotations:              Selector:  app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx
     Type:                     NodePort
     IP:                       10.43.33.70
     Port:                     http  80/TCP
     TargetPort:               80/TCP
     NodePort:                 http  32224/TCP
     Endpoints:                
     Port:                     https  443/TCP
     TargetPort:               443/TCP
     NodePort:                 https  31955/TCP
     Endpoints:                
     Port:                     teamspeak  9987/UDP
     TargetPort:               9987/UDP
     NodePort:                 teamspeak  31222/UDP
     Endpoints:                
     Session Affinity:         None
     External Traffic Policy:  Cluster
     Events:                   <none>
   #+end_src
   
   In this case the port that needs to be forwarded is the =31222= port (the
   =NodePort= for the 9987 =UDP= port).
* Testing an ansible collection                                     :ansible:
  :PROPERTIES:
  :EXPORT_FILE_NAME: testing-ansible-collections
  :EXPORT_DATE: 2020-05-08
  :END:
  When wanting to work on a collection for =ansible= it is (for now[fn:2]) important
  to check it out in a very specific folder structure or it will not be possible
  to run the tests for it.
  
  When trying to run =ansible-test integration= it will otherwise throw the
  following error:

  #+begin_src text
    ERROR: The current working directory must be at or below:

     - an Ansible collection: {...}/ansible_collections/{namespace}/{collection}/

    Current working directory: <some_other_dir>
  #+end_src

  The collection must be really placed inside a =subfolder= of a folder called
  =ansible_collections= and neither =namespace= nor =collection= can contain
  any symbols except =alphanumberics= or =underscores= (=[a-zA-Z0-9_]=):

  #+begin_src sh
    ansible_collections
    └── namespace
        └── collection
  #+end_src
  
  So if you want to work on an upstream collection (e.g. [[https://github.com/ansible-collections/community.general][community.general]]) you
  should create an intermediate folder =community= and clone the collection into
  the =general= folder (contrary to the default checkout which would be
  =community.general=):
  
  #+begin_src sh
    ansible_collections
    └── community
        └── general
  #+end_src
  
  Inside =general= you can now use run =ansible-test integration= to run
  the integration tests successfully:
  
  #+begin_src sh
    cd ansible_collections/community/general
    poetry init --name community.general --dependency=ansible --dependency=pyyaml --dependency=jinja2 -n
    poetry install
    poetry run ansible-test integration
  #+end_src
* Install kata-containers on K3S on aarch64            :kubernetes:raspberry:
  :PROPERTIES:
  :EXPORT_FILE_NAME: kata_containers_raspberry
  :EXPORT_DATE: 2020-05-21
  :END:
  To install =kata-containers= on a =raspberry-pi= and integrating it into
  =kubernetes= a few steps are currently required:

  - Install =kata-containers=
  - Setup integration into =containerd=
  - Setup integration into =kubernetes=
** Install =kata-containers=
   Currently the easiest (and apparently only) officially supported way to
   install =kata-containers= on an =aarch64= system is to use =snaps=.

   If you are using the =Ubuntu= =arm= version this is already included in the
   installation and you can install =kata-containers= with a single command[fn:4]:

   #+begin_src sh
     sudo snap install kata-containers --classic
   #+end_src
   
** Integration into containerd
   When running =k3s= there is no =containerd= binary as it is embedded into
   =k3s=, so configuration must be performed in the =k3s= configuration[fn:5].
   
   The whole process includes the following steps:

   - A =runtime= must be configured for =kata-containers= in =containerd=.
   - =shims= must be created for =containerd= to use =kata-containers=.
   - The node must be configured to be able to run =kata-container= workloads.
   - The kubernetes cluster must be aware that =kata= is a valid runtime.
   
   To configure a =runtime=, the configuration file for =k3s= is found under:
   =/var/lib/rancher/k3s/agent/etc/containerd/config.toml=, however it can not
   be edited as it will be regenerated every time =k3s= is restarted. Instead it
   [[https://rancher.com/docs/k3s/latest/en/advanced/][should be copied]] to
   =/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl= and that file
   can be modified.
   
   #+begin_src sh
     cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml{,.tmpl}
   #+end_src
   
   The content should be:

   #+begin_src toml
     [plugins.cri.containerd.runtimes.kata]
       runtime_type = "io.containerd.kata.v2"

     [kata.options]
       ConfigPath = "/etc/kata-containers/configuration.toml"
   #+end_src
   
   The configuration files for =/etc/kata-containers/*.toml= must be copied from
   the snap, because the default configuration will not be able to start on
   Raspberry Pis with < 4GB of memory, as =qemu= requests too much memory by
   default:

   #+begin_src sh
     mkdir -p /etc/kata-containers/
     cp /snap/kata-containers/current/usr/share/defaults/kata-containers/*.toml /etc/kata-containers/
   #+end_src
   
   Inside these configuration files (depending on the Pi) make sure to adjust the
   value of =default_memory= to something the hardware can handle. You should at
   least adjust the default =configuration.toml= and =configuration-qemu.toml=.
   
   In addition =shim= files must be created in =/usr/local/bin= to point to the
   =shim= binaries of =kata-containers= for =containerd= to pick it up.
   
   The following script will create all the shims (including some that might not
   actually be supported) - it was copied out of the =kata-deploy= project.

   #+begin_src sh
     #!/bin/bash
     shims=(
         "fc"
         "qemu"
         "qemu-virtiofs"
         "clh"
     )

     for shim in "${shims[@]}"; do
         shim_binary="containerd-shim-kata-${shim}-v2"
         shim_file="/usr/local/bin/${shim_binary}"
         shim_backup="/usr/local/bin/${shim_binary}.bak"

         if [ -f "${shim_file}" ]; then
             echo "warning: ${shim_binary} already exists" >&2
             if [ ! -f "${shim_backup}" ]; then
                 mv "${shim_file}" "${shim_backup}"
             else
                 rm "${shim_file}"
             fi
         fi
         cat << EOT | tee "$shim_file"
     #!/bin/bash
     KATA_CONF_FILE=/etc/kata-containers/configuration.toml /snap/kata-containers/current/usr/bin/containerd-shim-kata-v2 \$@
     EOT
         chmod +x "$shim_file"
     done

     # On the PI a default shim was also needed
     cp /usr/local/bin/containerd-shim-kata-qemu-v2 /usr/local/bin/containerd-shim-kata-v2
   #+end_src
   
   Now the =k3s-agent= must be restarted to reload the =containerd=
   configuration and we can test that kata-container runtime is working by
   running the following commands:

   #+begin_src sh
     systemctl restart k3s-agent
     ctr image pull docker.io/library/busybox:latest
     ctr run --runtime io.containerd.run.kata.v2 -t --rm docker.io/library/busybox:latest hello sh
   #+end_src
   
   If the tests succeed the node can now be labeled as supporting =kata-containers=:

   #+begin_src sh
     kubectl label node "$NODE_NAME" --overwrite katacontainers.io/kata-runtime=true
   #+end_src
   
   Now as a last step a new [[https://kubernetes.io/docs/concepts/containers/runtime-class/][RuntimeClass]] must be created for =kata= that can be
   used to force pods to be run using it:

   #+begin_src yaml
     apiVersion: node.k8s.io/v1beta1
     kind: RuntimeClass
     metadata:
       name: kata
     handler: kata
   #+end_src
   
   Now pods can use this runtime by specifying =runtimeClassName= inside the =spec=:

   #+begin_src yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: nginx-untrusted
     spec:
       runtimeClassName: kata
       containers:
       - name: nginx
         image: nginx
   #+end_src

* Footnotes

[fn:5] Most information in this section is a direct extract from [[https://github.com/kata-containers/packaging/pull/823][this PR]] adding =k3s= support to =kata-deploy=.

[fn:4] As documented [[https://github.com/kata-containers/packaging/blob/master/snap/README.md#initial-setup][here]].

[fn:3] For Ubuntu you will have to edit the file =/boot/firmware/cmdline.txt=
and add the options =cgroup_memory=1 cgroup_enable=memory= at the end of the
line for =k3s= (or =containers= in general) to work.

[fn:2] This might hopefully become easier in the future: https://github.com/ansible/ansible/issues/60215

[fn:1] See https://github.com/kubernetes/ingress-nginx/pull/3852 
